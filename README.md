# RobotEar
<p align="justify"> This is repository of proof of concept implementation for voice command processing for robot with minimal example of chain of thoughts and operating system command execution. </p>

https://github.com/PeterWaIIace/RobotEar/assets/40773550/685dc83e-c718-4cb8-b73c-b4e83d2e9a00

## Architecture
The RobotEar project utilizes various Python libraries and custom components to enable the following functionalities: 
- listening to audio input from a microphone,
- speech detection,
- execution of preprogrammed commands,
- interaction with the host operating system,
- generating synthesized text responses using a Python voice synthesizer (pyttsx3).


<p align="justify"> 
The core of the project is built on pyaudio, which leverages nonblocking callbacks to capture microphone data and save it to temporary .wav files. Speech recognition is performed using a local base model of Whisper, developed by OpenAI. The extracted user text from the speech undergoes two processing steps using OpenAI's language model (LLM): firstly, to execute commands on the operating system, and secondly, to reflect on its own response before delivering it to the user. The response generated by LLM is then synthesized using pyttsx3.
</p>

<p align="justify"> 
LLM interacts with the operating system through a command manager block, which maintains a preprogrammed list of accessible commands. This ensures predictable behavior and enables LLM to search through the list of available commands to make informed guesses from unintelligible speech. The command list can be easily expanded to accommodate specific system requirements.
</p>

<p align="justify"> 
The diagram below illustrates the design and relationships between the various components of the project:
</p>

![RobotEarDesign](https://github.com/PeterWaIIace/RobotEar/assets/40773550/00f12b0a-4f04-4609-a467-e503c5a7730a)

#### Minimal Chain of Thoughts - mCoT

<p align="justify"> 
During operation, every user input acts on LLM twice - this is called in this project Minimal Chain of Thoughts mCoT. As real chain of thoughts allows LLM to process complicated problems by being prompted itself, minimal implementation in RobotEar allows it to reflect on its own response, and take into account output from execution of command on operating system. This is far from chain of thoughts, however it implements very basic CoT behaviour.  
</p>

#### Adding new commands


<p align="justify"> 
System can be editted and expanded with new commands via simply editing `command.py`. It contains dictionary with commands names, which are used by LLM to filter user input, and instructions how to call commands use imperatively from python script (LLM has no access to it). 
</p>

This is example of dictionary for Windows:
```
self.config = {
    "get_time"   : "time /T",
    "get_date"   : "date /T"
}
```

## How to use it - Windows

##### 1. Install python dependencies: 
```
pip install -r requirements.txt
```
##### 2. Create config file and fill it with your OpenAi API keys
`config.json` - this file should be in same directory as main.py
```
{
    "apiKey": "somekey",
    "organization" : "someorg"
}
```

##### 3. Run script `main.py`:

```
python3 main.py
```
After running wait for downloading whisper model. Script should ask you which audio input you want to use for, choose the one which is the most suitable for your application.

## How to use it - Raspian [WiP]

It is still work in progress, code can run on raspberryPi4 64bit but I do not have microphone to test it with.

##### 1. Get RaspbianOS 64bit:

To use whispher which is based on pytorch, you will need 64bit operating system. It can be easily obtained with rapsberrypi imager https://www.raspberrypi.com/software/. 

##### 2. In raspbian install following requirements:
```
sudo apt-get install python3-pyaudio
sudo apt install espeak
```

##### 3. Install python dependencies: 
```
pip install -r requirements.txt
```
##### 4. Create config file and fill it with your OpenAi API keys
`config.json` - this file should be in same directory as main.py
```
{
    "apiKey": "somekey",
    "organization" : "someorg"
}
```

##### 5. Run script `main.py`:

```
python3 .\src\main.py
```
After running wait for downloading whisper model. Script should ask you which audio input you want to use for, choose the one which is the most suitable for your application.
